{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end Reusable ML Pipeline with Seldon and Kubeflow\n",
    "\n",
    "In this example we showcase how to build re-usable components to build an ML pipeline that can be trained and deployed at scale.\n",
    "\n",
    "We will automate content moderation on the Reddit comments in /r/science building a machine learning NLP model with the following components:\n",
    "\n",
    "![](img/completed-pipeline-deploy.jpg)\n",
    "\n",
    "This tutorial will break down in the following sections:\n",
    "\n",
    "1) Run all the services (Kubeflow and Seldon)\n",
    "\n",
    "2) Test and build all our reusable pipeline steps\n",
    "\n",
    "3) Use Kubeflow to Train the Pipeline and Deploy to Seldon\n",
    "\n",
    "5) Test Seldon Deployed ML REST Endpoints\n",
    "\n",
    "6) Visualise Seldon's Production ML Pipelines\n",
    "\n",
    "# Before you start\n",
    "Make sure you install the following dependencies, as they are critical for this example to work:\n",
    "\n",
    "* Helm v2.13.1+\n",
    "* A Kubernetes cluster running v1.13 or above (minkube / docker-for-windows work well if enough RAM)\n",
    "* kubectl v1.14+\n",
    "* ksonnet v0.13.1+\n",
    "* kfctl 0.5.1 - Please use this exact version as there are major changes every few months\n",
    "* Python 3.6+\n",
    "* Python DEV requirements (we'll install them below)\n",
    "\n",
    "Let's get started! ðŸš€ðŸ”¥ We will be building the end-to-end pipeline below:\n",
    "\n",
    "![](img/kubeflow-seldon-nlp-full.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python-dateutil\r\n",
      "https://storage.googleapis.com/ml-pipeline/release/0.1.20/kfp.tar.gz\r\n",
      "kubernetes\r\n",
      "click\r\n",
      "seldon_core\r\n",
      "numpy\r\n"
     ]
    }
   ],
   "source": [
    "!cat requirements-dev.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://storage.googleapis.com/ml-pipeline/release/0.1.20/kfp.tar.gz (from -r requirements-dev.txt (line 2))\n",
      "  Using cached https://storage.googleapis.com/ml-pipeline/release/0.1.20/kfp.tar.gz\n",
      "Requirement already satisfied (use --upgrade to upgrade): kfp==0.1.20 from https://storage.googleapis.com/ml-pipeline/release/0.1.20/kfp.tar.gz in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from -r requirements-dev.txt (line 2))\n",
      "Requirement already satisfied: python-dateutil in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from -r requirements-dev.txt (line 1)) (2.8.0)\n",
      "Requirement already satisfied: kubernetes in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from -r requirements-dev.txt (line 3)) (9.0.0)\n",
      "Requirement already satisfied: click in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from -r requirements-dev.txt (line 4)) (7.0)\n",
      "Requirement already satisfied: seldon_core in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from -r requirements-dev.txt (line 5)) (0.2.7.1)\n",
      "Requirement already satisfied: numpy in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from -r requirements-dev.txt (line 6)) (1.16.3)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.15 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from kfp==0.1.20->-r requirements-dev.txt (line 2)) (1.24.1)\n",
      "Requirement already satisfied: six>=1.10 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from kfp==0.1.20->-r requirements-dev.txt (line 2)) (1.12.0)\n",
      "Requirement already satisfied: certifi in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from kfp==0.1.20->-r requirements-dev.txt (line 2)) (2019.3.9)\n",
      "Requirement already satisfied: PyYAML in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from kfp==0.1.20->-r requirements-dev.txt (line 2)) (5.1)\n",
      "Requirement already satisfied: google-cloud-storage>=1.13.0 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from kfp==0.1.20->-r requirements-dev.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: PyJWT>=1.6.4 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from kfp==0.1.20->-r requirements-dev.txt (line 2)) (1.7.1)\n",
      "Requirement already satisfied: cryptography>=2.4.2 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from kfp==0.1.20->-r requirements-dev.txt (line 2)) (2.6.1)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from kfp==0.1.20->-r requirements-dev.txt (line 2)) (1.6.3)\n",
      "Requirement already satisfied: requests_toolbelt>=0.8.0 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from kfp==0.1.20->-r requirements-dev.txt (line 2)) (0.9.1)\n",
      "Requirement already satisfied: kfp-server-api<0.1.19,>=0.1.18 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from kfp==0.1.20->-r requirements-dev.txt (line 2)) (0.1.18.2)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from kubernetes->-r requirements-dev.txt (line 3)) (41.0.0)\n",
      "Requirement already satisfied: requests in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from kubernetes->-r requirements-dev.txt (line 3)) (2.21.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from kubernetes->-r requirements-dev.txt (line 3)) (0.56.0)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from kubernetes->-r requirements-dev.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: tensorflow in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from seldon_core->-r requirements-dev.txt (line 5)) (1.13.1)\n",
      "Requirement already satisfied: flask in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from seldon_core->-r requirements-dev.txt (line 5)) (1.0.3)\n",
      "Requirement already satisfied: flatbuffers in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from seldon_core->-r requirements-dev.txt (line 5)) (1.11)\n",
      "Requirement already satisfied: protobuf in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from seldon_core->-r requirements-dev.txt (line 5)) (3.7.1)\n",
      "Requirement already satisfied: flask-cors in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from seldon_core->-r requirements-dev.txt (line 5)) (3.0.7)\n",
      "Requirement already satisfied: jaeger-client==3.13.0 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from seldon_core->-r requirements-dev.txt (line 5)) (3.13.0)\n",
      "Requirement already satisfied: grpcio in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from seldon_core->-r requirements-dev.txt (line 5)) (1.21.1)\n",
      "Requirement already satisfied: redis in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from seldon_core->-r requirements-dev.txt (line 5)) (3.2.1)\n",
      "Requirement already satisfied: grpcio-opentracing in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from seldon_core->-r requirements-dev.txt (line 5)) (1.1.4)\n",
      "Requirement already satisfied: Flask-OpenTracing==0.2.0 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from seldon_core->-r requirements-dev.txt (line 5)) (0.2.0)\n",
      "Requirement already satisfied: opentracing<2,>=1.2.2 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from seldon_core->-r requirements-dev.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.6.0 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.1.20->-r requirements-dev.txt (line 2)) (1.11.0)\n",
      "Requirement already satisfied: google-resumable-media>=0.3.1 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.1.20->-r requirements-dev.txt (line 2)) (0.3.2)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.0 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.1.20->-r requirements-dev.txt (line 2)) (1.0.0)\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from cryptography>=2.4.2->kfp==0.1.20->-r requirements-dev.txt (line 2)) (0.24.0)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from cryptography>=2.4.2->kfp==0.1.20->-r requirements-dev.txt (line 2)) (1.12.2)\n",
      "Requirement already satisfied: cachetools>=2.0.0 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.1.20->-r requirements-dev.txt (line 2)) (3.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.1.20->-r requirements-dev.txt (line 2)) (0.2.5)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.1.20->-r requirements-dev.txt (line 2)) (4.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from requests->kubernetes->-r requirements-dev.txt (line 3)) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from requests->kubernetes->-r requirements-dev.txt (line 3)) (2.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from requests-oauthlib->kubernetes->-r requirements-dev.txt (line 3)) (3.0.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from tensorflow->seldon_core->-r requirements-dev.txt (line 5)) (0.33.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from tensorflow->seldon_core->-r requirements-dev.txt (line 5)) (1.0.7)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from tensorflow->seldon_core->-r requirements-dev.txt (line 5)) (1.0.9)\n",
      "Requirement already satisfied: astor>=0.6.0 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from tensorflow->seldon_core->-r requirements-dev.txt (line 5)) (0.8.0)\n",
      "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from tensorflow->seldon_core->-r requirements-dev.txt (line 5)) (1.13.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gast>=0.2.0 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from tensorflow->seldon_core->-r requirements-dev.txt (line 5)) (0.2.2)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from tensorflow->seldon_core->-r requirements-dev.txt (line 5)) (0.7.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from tensorflow->seldon_core->-r requirements-dev.txt (line 5)) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from tensorflow->seldon_core->-r requirements-dev.txt (line 5)) (1.13.0)\n",
      "Requirement already satisfied: Werkzeug>=0.14 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from flask->seldon_core->-r requirements-dev.txt (line 5)) (0.15.4)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from flask->seldon_core->-r requirements-dev.txt (line 5)) (1.1.0)\n",
      "Requirement already satisfied: Jinja2>=2.10 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from flask->seldon_core->-r requirements-dev.txt (line 5)) (2.10.1)\n",
      "Requirement already satisfied: thrift in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from jaeger-client==3.13.0->seldon_core->-r requirements-dev.txt (line 5)) (0.11.0)\n",
      "Requirement already satisfied: threadloop<2,>=1 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from jaeger-client==3.13.0->seldon_core->-r requirements-dev.txt (line 5)) (1.0.2)\n",
      "Requirement already satisfied: tornado<5,>=4.3 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from jaeger-client==3.13.0->seldon_core->-r requirements-dev.txt (line 5)) (4.5.3)\n",
      "Requirement already satisfied: pytz in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.6.0->google-cloud-storage>=1.13.0->kfp==0.1.20->-r requirements-dev.txt (line 2)) (2019.1)\n",
      "Requirement already satisfied: googleapis-common-protos!=1.5.4,<2.0dev,>=1.5.3 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.6.0->google-cloud-storage>=1.13.0->kfp==0.1.20->-r requirements-dev.txt (line 2)) (1.6.0)\n",
      "Requirement already satisfied: pycparser in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.4.2->kfp==0.1.20->-r requirements-dev.txt (line 2)) (2.19)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.1 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp==0.1.20->-r requirements-dev.txt (line 2)) (0.4.5)\n",
      "Requirement already satisfied: h5py in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from keras-applications>=1.0.6->tensorflow->seldon_core->-r requirements-dev.txt (line 5)) (2.9.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow->seldon_core->-r requirements-dev.txt (line 5)) (3.1.1)\n",
      "Requirement already satisfied: mock>=2.0.0 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow->seldon_core->-r requirements-dev.txt (line 5)) (3.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/Seldon/miniconda3/lib/python3.7/site-packages (from Jinja2>=2.10->flask->seldon_core->-r requirements-dev.txt (line 5)) (1.1.1)\n",
      "Building wheels for collected packages: kfp\n",
      "  Building wheel for kfp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /private/var/folders/3f/hgftp3157t7cn_cb0l16vpk00000gp/T/pip-ephem-wheel-cache-n629w35g/wheels/ae/bb/02/32b1356ee756181099d8f1b0950ac6567cb2b38e71b48f02e8\n",
      "Successfully built kfp\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements-dev.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Run all the services (Kubeflow and Seldon)\n",
    "Kubeflow's CLI allows us to create a project which will allow us to build the configuration we need to deploy our kubeflow and seldon clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app.yaml\r\n"
     ]
    }
   ],
   "source": [
    "!kfctl init kubeflow-seldon\n",
    "!ls kubeflow-seldon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the following commands to basically launch our Kubeflow cluster with all its components. \n",
    "\n",
    "It may take a while to download all the images for Kubeflow so feel free to make yourself a cup of â˜•.\n",
    "\n",
    "If you have a terminal you can see how the containers are created in real-time by running `kubectl get pods -n kubeflow -w`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=\"2019-07-03T12:15:59+01:00\" level=info msg=\"reading from /Users/Seldon/seldon-core/examples/kubeflow/kubeflow-seldon/app.yaml\" filename=\"coordinator/coordinator.go:341\"\n",
      "time=\"2019-07-03T12:15:59+01:00\" level=info msg=\"reading from /Users/Seldon/seldon-core/examples/kubeflow/kubeflow-seldon/app.yaml\" filename=\"coordinator/coordinator.go:341\"\n",
      "time=\"2019-07-03T12:15:59+01:00\" level=info msg=\"Ksonnet.Generate Name kubeflow-seldon AppDir /Users/Seldon/seldon-core/examples/kubeflow/kubeflow-seldon Platform \" filename=\"ksonnet/ksonnet.go:369\"\n",
      "time=\"2019-07-03T12:16:00+01:00\" level=info msg=\"Creating environment \\\"default\\\" with namespace \\\"kubeflow\\\", pointing to \\\"version:v1.13.6\\\" cluster at address \\\"https://35.222.71.105\\\"\" filename=\"env/create.go:77\"\n",
      "time=\"2019-07-03T12:16:03+01:00\" level=info msg=\"Generating ksonnet-lib data at path '/Users/Seldon/seldon-core/examples/kubeflow/kubeflow-seldon/ks_app/lib/ksonnet-lib/v1.13.6'\" filename=\"lib/lib.go:148\"\n",
      "time=\"2019-07-03T12:16:03+01:00\" level=info msg=\"Successfully initialized the app kubeflow-seldon.\" filename=\"ksonnet/ksonnet.go:505\"\n",
      "time=\"2019-07-03T12:16:03+01:00\" level=info msg=\"App kubeflow-seldon add registry kubeflow URI /Users/Seldon/seldon-core/examples/kubeflow/kubeflow-seldon/.cache/v0.5.1/kubeflow\" filename=\"ksonnet/ksonnet.go:621\"\n",
      "time=\"2019-07-03T12:16:03+01:00\" level=info msg=\"Retrieved 6 files\" filename=\"registry/cache.go:114\"\n",
      "time=\"2019-07-03T12:16:03+01:00\" level=info msg=\"Retrieved 22 files\" filename=\"registry/cache.go:114\"\n",
      "time=\"2019-07-03T12:16:03+01:00\" level=info msg=\"Retrieved 9 files\" filename=\"registry/cache.go:114\"\n",
      "time=\"2019-07-03T12:16:04+01:00\" level=info msg=\"Retrieved 35 files\" filename=\"registry/cache.go:114\"\n",
      "time=\"2019-07-03T12:16:04+01:00\" level=info msg=\"Retrieved 49 files\" filename=\"registry/cache.go:114\"\n",
      "time=\"2019-07-03T12:16:04+01:00\" level=info msg=\"Retrieved 6 files\" filename=\"registry/cache.go:114\"\n",
      "time=\"2019-07-03T12:16:04+01:00\" level=info msg=\"Retrieved 4 files\" filename=\"registry/cache.go:114\"\n",
      "time=\"2019-07-03T12:16:04+01:00\" level=info msg=\"Retrieved 4 files\" filename=\"registry/cache.go:114\"\n",
      "time=\"2019-07-03T12:16:04+01:00\" level=info msg=\"Retrieved 9 files\" filename=\"registry/cache.go:114\"\n",
      "time=\"2019-07-03T12:16:04+01:00\" level=info msg=\"Retrieved 14 files\" filename=\"registry/cache.go:114\"\n",
      "time=\"2019-07-03T12:16:04+01:00\" level=info msg=\"Retrieved 6 files\" filename=\"registry/cache.go:114\"\n",
      "time=\"2019-07-03T12:16:04+01:00\" level=info msg=\"Retrieved 17 files\" filename=\"registry/cache.go:114\"\n",
      "time=\"2019-07-03T12:16:04+01:00\" level=info msg=\"Retrieved 8 files\" filename=\"registry/cache.go:114\"\n",
      "time=\"2019-07-03T12:16:04+01:00\" level=info msg=\"Retrieved 12 files\" filename=\"registry/cache.go:114\"\n",
      "time=\"2019-07-03T12:16:04+01:00\" level=info msg=\"Retrieved 4 files\" filename=\"registry/cache.go:114\"\n",
      "time=\"2019-07-03T12:16:04+01:00\" level=info msg=\"Creating Component: ambassador ...\" filename=\"ksonnet/ksonnet.go:207\"\n",
      "time=\"2019-07-03T12:16:04+01:00\" level=info msg=\"Args: [ambassador ambassador]\" filename=\"ksonnet/ksonnet.go:208\"\n",
      "time=\"2019-07-03T12:16:04+01:00\" level=info msg=\"Writing component at '/Users/Seldon/seldon-core/examples/kubeflow/kubeflow-seldon/ks_app/components/ambassador.jsonnet'\" filename=\"component/create.go:92\"\n",
      "time=\"2019-07-03T12:16:04+01:00\" level=info msg=\"Creating Component: argo ...\" filename=\"ksonnet/ksonnet.go:207\"\n",
      "time=\"2019-07-03T12:16:04+01:00\" level=info msg=\"Args: [argo argo]\" filename=\"ksonnet/ksonnet.go:208\"\n",
      "time=\"2019-07-03T12:16:04+01:00\" level=info msg=\"Writing component at '/Users/Seldon/seldon-core/examples/kubeflow/kubeflow-seldon/ks_app/components/argo.jsonnet'\" filename=\"component/create.go:92\"\n",
      "time=\"2019-07-03T12:16:04+01:00\" level=info msg=\"Creating Component: centraldashboard ...\" filename=\"ksonnet/ksonnet.go:207\"\n",
      "time=\"2019-07-03T12:16:04+01:00\" level=info msg=\"Args: [centraldashboard centraldashboard]\" filename=\"ksonnet/ksonnet.go:208\"\n",
      "time=\"2019-07-03T12:16:04+01:00\" level=info msg=\"Writing component at '/Users/Seldon/seldon-core/examples/kubeflow/kubeflow-seldon/ks_app/components/centraldashboard.jsonnet'\" filename=\"component/create.go:92\"\n",
      "time=\"2019-07-03T12:16:04+01:00\" level=info msg=\"Creating Component: jupyter-web-app ...\" filename=\"ksonnet/ksonnet.go:207\"\n",
      "time=\"2019-07-03T12:16:04+01:00\" level=info msg=\"Args: [jupyter-web-app jupyter-web-app]\" filename=\"ksonnet/ksonnet.go:208\"\n",
      "time=\"2019-07-03T12:16:04+01:00\" level=info msg=\"Writing component at '/Users/Seldon/seldon-core/examples/kubeflow/kubeflow-seldon/ks_app/components/jupyter-web-app.jsonnet'\" filename=\"component/create.go:92\"\n",
      "time=\"2019-07-03T12:16:04+01:00\" level=info msg=\"Creating Component: katib ...\" filename=\"ksonnet/ksonnet.go:207\"\n",
      "time=\"2019-07-03T12:16:04+01:00\" level=info msg=\"Args: [katib katib]\" filename=\"ksonnet/ksonnet.go:208\"\n",
      "time=\"2019-07-03T12:16:04+01:00\" level=info msg=\"Writing component at '/Users/Seldon/seldon-core/examples/kubeflow/kubeflow-seldon/ks_app/components/katib.jsonnet'\" filename=\"component/create.go:92\"\n",
      "time=\"2019-07-03T12:16:04+01:00\" level=info msg=\"Creating Component: metacontroller ...\" filename=\"ksonnet/ksonnet.go:207\"\n",
      "time=\"2019-07-03T12:16:04+01:00\" level=info msg=\"Args: [metacontroller metacontroller]\" filename=\"ksonnet/ksonnet.go:208\"\n",
      "time=\"2019-07-03T12:16:04+01:00\" level=info msg=\"Writing component at '/Users/Seldon/seldon-core/examples/kubeflow/kubeflow-seldon/ks_app/components/metacontroller.jsonnet'\" filename=\"component/create.go:92\"\n",
      "time=\"2019-07-03T12:16:04+01:00\" level=info msg=\"Creating Component: notebook-controller ...\" filename=\"ksonnet/ksonnet.go:207\"\n",
      "time=\"2019-07-03T12:16:04+01:00\" level=info msg=\"Args: [notebook-controller notebook-controller]\" filename=\"ksonnet/ksonnet.go:208\"\n",
      "time=\"2019-07-03T12:16:05+01:00\" level=info msg=\"Writing component at '/Users/Seldon/seldon-core/examples/kubeflow/kubeflow-seldon/ks_app/components/notebook-controller.jsonnet'\" filename=\"component/create.go:92\"\n",
      "time=\"2019-07-03T12:16:05+01:00\" level=info msg=\"Creating Component: pipeline ...\" filename=\"ksonnet/ksonnet.go:207\"\n",
      "time=\"2019-07-03T12:16:05+01:00\" level=info msg=\"Args: [pipeline pipeline]\" filename=\"ksonnet/ksonnet.go:208\"\n",
      "time=\"2019-07-03T12:16:05+01:00\" level=info msg=\"Writing component at '/Users/Seldon/seldon-core/examples/kubeflow/kubeflow-seldon/ks_app/components/pipeline.jsonnet'\" filename=\"component/create.go:92\"\n",
      "time=\"2019-07-03T12:16:05+01:00\" level=info msg=\"Creating Component: pytorch-operator ...\" filename=\"ksonnet/ksonnet.go:207\"\n",
      "time=\"2019-07-03T12:16:05+01:00\" level=info msg=\"Args: [pytorch-operator pytorch-operator]\" filename=\"ksonnet/ksonnet.go:208\"\n",
      "time=\"2019-07-03T12:16:05+01:00\" level=info msg=\"Writing component at '/Users/Seldon/seldon-core/examples/kubeflow/kubeflow-seldon/ks_app/components/pytorch-operator.jsonnet'\" filename=\"component/create.go:92\"\n",
      "time=\"2019-07-03T12:16:05+01:00\" level=info msg=\"Creating Component: tensorboard ...\" filename=\"ksonnet/ksonnet.go:207\"\n",
      "time=\"2019-07-03T12:16:05+01:00\" level=info msg=\"Args: [tensorboard tensorboard]\" filename=\"ksonnet/ksonnet.go:208\"\n",
      "time=\"2019-07-03T12:16:05+01:00\" level=info msg=\"Writing component at '/Users/Seldon/seldon-core/examples/kubeflow/kubeflow-seldon/ks_app/components/tensorboard.jsonnet'\" filename=\"component/create.go:92\"\n",
      "time=\"2019-07-03T12:16:05+01:00\" level=info msg=\"Creating Component: tf-job-operator ...\" filename=\"ksonnet/ksonnet.go:207\"\n",
      "time=\"2019-07-03T12:16:05+01:00\" level=info msg=\"Args: [tf-job-operator tf-job-operator]\" filename=\"ksonnet/ksonnet.go:208\"\n",
      "time=\"2019-07-03T12:16:05+01:00\" level=info msg=\"Writing component at '/Users/Seldon/seldon-core/examples/kubeflow/kubeflow-seldon/ks_app/components/tf-job-operator.jsonnet'\" filename=\"component/create.go:92\"\n",
      "time=\"2019-07-03T12:16:07+01:00\" level=info msg=\"deploying kubeflow application\" filename=\"cmd/apply.go:35\"\n",
      "time=\"2019-07-03T12:16:07+01:00\" level=info msg=\"reading from /Users/Seldon/seldon-core/examples/kubeflow/kubeflow-seldon/app.yaml\" filename=\"coordinator/coordinator.go:341\"\n",
      "time=\"2019-07-03T12:16:07+01:00\" level=info msg=\"reading from /Users/Seldon/seldon-core/examples/kubeflow/kubeflow-seldon/app.yaml\" filename=\"coordinator/coordinator.go:341\"\n",
      "time=\"2019-07-03T12:16:07+01:00\" level=info msg=\"namespace: kubeflow\" filename=\"ksonnet/ksonnet.go:109\"\n",
      "time=\"2019-07-03T12:16:07+01:00\" level=info msg=\"Creating namespace: kubeflow\" filename=\"ksonnet/ksonnet.go:112\"\n",
      "time=\"2019-07-03T12:16:19+01:00\" level=info msg=\"Applying services kubeflow.ambassador\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:16:19+01:00\" level=info msg=\"Creating non-existent services kubeflow.ambassador\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:16:19+01:00\" level=info msg=\"Applying services kubeflow.ambassador-admin\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:16:20+01:00\" level=info msg=\"Creating non-existent services kubeflow.ambassador-admin\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:16:20+01:00\" level=info msg=\"Applying clusterroles ambassador\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:16:20+01:00\" level=info msg=\"Creating non-existent clusterroles ambassador\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:16:21+01:00\" level=info msg=\"Applying serviceaccounts kubeflow.ambassador\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:16:21+01:00\" level=info msg=\"Creating non-existent serviceaccounts kubeflow.ambassador\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:16:22+01:00\" level=info msg=\"Applying clusterrolebindings ambassador\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:16:22+01:00\" level=info msg=\"Creating non-existent clusterrolebindings ambassador\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:16:22+01:00\" level=info msg=\"Applying deployments kubeflow.ambassador\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:16:22+01:00\" level=info msg=\"Creating non-existent deployments kubeflow.ambassador\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:16:23+01:00\" level=info msg=\"Component ambassador apply succeeded\" filename=\"ksonnet/ksonnet.go:171\"\n",
      "time=\"2019-07-03T12:16:33+01:00\" level=info msg=\"Applying customresourcedefinitions workflows.argoproj.io\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:16:33+01:00\" level=info msg=\"Creating non-existent customresourcedefinitions workflows.argoproj.io\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:16:33+01:00\" level=info msg=\"Applying clusterrolebindings argo\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:16:34+01:00\" level=info msg=\"Creating non-existent clusterrolebindings argo\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:16:34+01:00\" level=info msg=\"Applying serviceaccounts kubeflow.argo-ui\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:16:34+01:00\" level=info msg=\"Creating non-existent serviceaccounts kubeflow.argo-ui\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:16:35+01:00\" level=info msg=\"Applying services kubeflow.argo-ui\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:16:35+01:00\" level=info msg=\"Creating non-existent services kubeflow.argo-ui\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:16:35+01:00\" level=info msg=\"Applying configmaps kubeflow.workflow-controller-configmap\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:16:35+01:00\" level=info msg=\"Creating non-existent configmaps kubeflow.workflow-controller-configmap\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:16:36+01:00\" level=info msg=\"Applying serviceaccounts kubeflow.argo\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:16:36+01:00\" level=info msg=\"Creating non-existent serviceaccounts kubeflow.argo\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:16:36+01:00\" level=info msg=\"Applying clusterroles argo\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:16:36+01:00\" level=info msg=\"Creating non-existent clusterroles argo\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:16:37+01:00\" level=info msg=\"Applying clusterroles argo-ui\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:16:37+01:00\" level=info msg=\"Creating non-existent clusterroles argo-ui\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:16:37+01:00\" level=info msg=\"Applying clusterrolebindings argo-ui\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:16:37+01:00\" level=info msg=\"Creating non-existent clusterrolebindings argo-ui\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:16:38+01:00\" level=info msg=\"Applying deployments kubeflow.workflow-controller\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:16:38+01:00\" level=info msg=\"Creating non-existent deployments kubeflow.workflow-controller\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:16:38+01:00\" level=info msg=\"Applying deployments kubeflow.argo-ui\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:16:38+01:00\" level=info msg=\"Creating non-existent deployments kubeflow.argo-ui\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:16:38+01:00\" level=info msg=\"Component argo apply succeeded\" filename=\"ksonnet/ksonnet.go:171\"\n",
      "time=\"2019-07-03T12:16:48+01:00\" level=info msg=\"Applying clusterrolebindings centraldashboard\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:16:48+01:00\" level=info msg=\"Creating non-existent clusterrolebindings centraldashboard\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:16:49+01:00\" level=info msg=\"Applying services kubeflow.centraldashboard\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:16:49+01:00\" level=info msg=\"Creating non-existent services kubeflow.centraldashboard\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:16:49+01:00\" level=info msg=\"Applying serviceaccounts kubeflow.centraldashboard\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:16:49+01:00\" level=info msg=\"Creating non-existent serviceaccounts kubeflow.centraldashboard\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:16:50+01:00\" level=info msg=\"Applying roles kubeflow.centraldashboard\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:16:50+01:00\" level=info msg=\"Creating non-existent roles kubeflow.centraldashboard\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:16:50+01:00\" level=info msg=\"Applying rolebindings kubeflow.centraldashboard\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:16:50+01:00\" level=info msg=\"Creating non-existent rolebindings kubeflow.centraldashboard\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:16:51+01:00\" level=info msg=\"Applying clusterroles centraldashboard\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:16:51+01:00\" level=info msg=\"Creating non-existent clusterroles centraldashboard\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:16:51+01:00\" level=info msg=\"Applying deployments kubeflow.centraldashboard\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:16:51+01:00\" level=info msg=\"Creating non-existent deployments kubeflow.centraldashboard\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:16:52+01:00\" level=info msg=\"Component centraldashboard apply succeeded\" filename=\"ksonnet/ksonnet.go:171\"\n",
      "time=\"2019-07-03T12:17:02+01:00\" level=info msg=\"Applying services kubeflow.jupyter-web-app\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:02+01:00\" level=info msg=\"Creating non-existent services kubeflow.jupyter-web-app\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:03+01:00\" level=info msg=\"Applying deployments kubeflow.jupyter-web-app\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:03+01:00\" level=info msg=\"Creating non-existent deployments kubeflow.jupyter-web-app\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:03+01:00\" level=info msg=\"Applying configmaps kubeflow.jupyter-web-app-config\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:03+01:00\" level=info msg=\"Creating non-existent configmaps kubeflow.jupyter-web-app-config\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:04+01:00\" level=info msg=\"Applying serviceaccounts kubeflow.jupyter-web-app\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:04+01:00\" level=info msg=\"Creating non-existent serviceaccounts kubeflow.jupyter-web-app\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:04+01:00\" level=info msg=\"Applying clusterrolebindings jupyter-web-app-binding\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:04+01:00\" level=info msg=\"Creating non-existent clusterrolebindings jupyter-web-app-binding\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:05+01:00\" level=info msg=\"Applying clusterroles jupyter-web-app-cluster-role\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:05+01:00\" level=info msg=\"Creating non-existent clusterroles jupyter-web-app-cluster-role\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:05+01:00\" level=info msg=\"Applying serviceaccounts kubeflow.jupyter-notebook\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:05+01:00\" level=info msg=\"Creating non-existent serviceaccounts kubeflow.jupyter-notebook\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:06+01:00\" level=info msg=\"Applying roles kubeflow.jupyter-notebook-role\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:06+01:00\" level=info msg=\"Creating non-existent roles kubeflow.jupyter-notebook-role\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:06+01:00\" level=info msg=\"Applying rolebindings kubeflow.jupyter-notebook-role-binding\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:06+01:00\" level=info msg=\"Creating non-existent rolebindings kubeflow.jupyter-notebook-role-binding\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:07+01:00\" level=info msg=\"Component jupyter-web-app apply succeeded\" filename=\"ksonnet/ksonnet.go:171\"\n",
      "time=\"2019-07-03T12:17:16+01:00\" level=info msg=\"Applying clusterroles metrics-collector\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:16+01:00\" level=info msg=\"Creating non-existent clusterroles metrics-collector\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:16+01:00\" level=info msg=\"Applying services kubeflow.studyjob-controller\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:16+01:00\" level=info msg=\"Creating non-existent services kubeflow.studyjob-controller\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:17+01:00\" level=info msg=\"Applying services kubeflow.vizier-db\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:17+01:00\" level=info msg=\"Creating non-existent services kubeflow.vizier-db\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:17+01:00\" level=info msg=\"Applying persistentvolumeclaims kubeflow.katib-mysql\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:17+01:00\" level=info msg=\"Creating non-existent persistentvolumeclaims kubeflow.katib-mysql\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:18+01:00\" level=info msg=\"Applying clusterrolebindings studyjob-controller\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:18+01:00\" level=info msg=\"Creating non-existent clusterrolebindings studyjob-controller\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:18+01:00\" level=info msg=\"Applying secrets kubeflow.vizier-db-secrets\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:18+01:00\" level=info msg=\"Creating non-existent secrets kubeflow.vizier-db-secrets\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:19+01:00\" level=info msg=\"Applying clusterroles vizier-core\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:19+01:00\" level=info msg=\"Creating non-existent clusterroles vizier-core\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:19+01:00\" level=info msg=\"Applying clusterrolebindings vizier-core\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:19+01:00\" level=info msg=\"Creating non-existent clusterrolebindings vizier-core\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:20+01:00\" level=info msg=\"Applying serviceaccounts kubeflow.vizier-core\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:20+01:00\" level=info msg=\"Creating non-existent serviceaccounts kubeflow.vizier-core\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:20+01:00\" level=info msg=\"Applying services kubeflow.vizier-core-rest\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:21+01:00\" level=info msg=\"Creating non-existent services kubeflow.vizier-core-rest\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:21+01:00\" level=info msg=\"Applying serviceaccounts kubeflow.studyjob-controller\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:21+01:00\" level=info msg=\"Creating non-existent serviceaccounts kubeflow.studyjob-controller\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:21+01:00\" level=info msg=\"Applying services kubeflow.katib-ui\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:22+01:00\" level=info msg=\"Creating non-existent services kubeflow.katib-ui\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:22+01:00\" level=info msg=\"Applying clusterroles studyjob-controller\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:22+01:00\" level=info msg=\"Creating non-existent clusterroles studyjob-controller\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:22+01:00\" level=info msg=\"Applying clusterroles katib-ui\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:23+01:00\" level=info msg=\"Creating non-existent clusterroles katib-ui\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:23+01:00\" level=info msg=\"Applying clusterrolebindings katib-ui\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:23+01:00\" level=info msg=\"Creating non-existent clusterrolebindings katib-ui\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:23+01:00\" level=info msg=\"Applying serviceaccounts kubeflow.katib-ui\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:23+01:00\" level=info msg=\"Creating non-existent serviceaccounts kubeflow.katib-ui\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:24+01:00\" level=info msg=\"Applying services kubeflow.vizier-suggestion-random\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:24+01:00\" level=info msg=\"Creating non-existent services kubeflow.vizier-suggestion-random\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:24+01:00\" level=info msg=\"Applying services kubeflow.vizier-core\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:25+01:00\" level=info msg=\"Creating non-existent services kubeflow.vizier-core\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:25+01:00\" level=info msg=\"Applying services kubeflow.vizier-suggestion-grid\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:25+01:00\" level=info msg=\"Creating non-existent services kubeflow.vizier-suggestion-grid\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:25+01:00\" level=info msg=\"Applying configmaps kubeflow.metricscollector-template\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:26+01:00\" level=info msg=\"Creating non-existent configmaps kubeflow.metricscollector-template\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:26+01:00\" level=info msg=\"Applying services kubeflow.vizier-suggestion-hyperband\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:26+01:00\" level=info msg=\"Creating non-existent services kubeflow.vizier-suggestion-hyperband\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:26+01:00\" level=info msg=\"Applying clusterrolebindings metrics-collector\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:26+01:00\" level=info msg=\"Creating non-existent clusterrolebindings metrics-collector\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:27+01:00\" level=info msg=\"Applying services kubeflow.vizier-suggestion-bayesianoptimization\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:27+01:00\" level=info msg=\"Creating non-existent services kubeflow.vizier-suggestion-bayesianoptimization\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:27+01:00\" level=info msg=\"Applying serviceaccounts kubeflow.metrics-collector\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:28+01:00\" level=info msg=\"Creating non-existent serviceaccounts kubeflow.metrics-collector\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:28+01:00\" level=info msg=\"Applying customresourcedefinitions studyjobs.kubeflow.org\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:28+01:00\" level=info msg=\"Creating non-existent customresourcedefinitions studyjobs.kubeflow.org\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:28+01:00\" level=info msg=\"Applying configmaps kubeflow.worker-template\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:29+01:00\" level=info msg=\"Creating non-existent configmaps kubeflow.worker-template\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:29+01:00\" level=info msg=\"Applying deployments kubeflow.vizier-suggestion-bayesianoptimization\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:29+01:00\" level=info msg=\"Creating non-existent deployments kubeflow.vizier-suggestion-bayesianoptimization\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:30+01:00\" level=info msg=\"Applying deployments kubeflow.vizier-suggestion-hyperband\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:30+01:00\" level=info msg=\"Creating non-existent deployments kubeflow.vizier-suggestion-hyperband\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:30+01:00\" level=info msg=\"Applying deployments kubeflow.vizier-suggestion-grid\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:30+01:00\" level=info msg=\"Creating non-existent deployments kubeflow.vizier-suggestion-grid\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:31+01:00\" level=info msg=\"Applying deployments kubeflow.katib-ui\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:31+01:00\" level=info msg=\"Creating non-existent deployments kubeflow.katib-ui\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:31+01:00\" level=info msg=\"Applying deployments kubeflow.vizier-core-rest\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:31+01:00\" level=info msg=\"Creating non-existent deployments kubeflow.vizier-core-rest\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:32+01:00\" level=info msg=\"Applying deployments kubeflow.vizier-db\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:32+01:00\" level=info msg=\"Creating non-existent deployments kubeflow.vizier-db\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:32+01:00\" level=info msg=\"Applying deployments kubeflow.studyjob-controller\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:32+01:00\" level=info msg=\"Creating non-existent deployments kubeflow.studyjob-controller\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:33+01:00\" level=info msg=\"Applying deployments kubeflow.vizier-core\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:33+01:00\" level=info msg=\"Creating non-existent deployments kubeflow.vizier-core\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:33+01:00\" level=info msg=\"Applying deployments kubeflow.vizier-suggestion-random\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:33+01:00\" level=info msg=\"Creating non-existent deployments kubeflow.vizier-suggestion-random\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:33+01:00\" level=info msg=\"Component katib apply succeeded\" filename=\"ksonnet/ksonnet.go:171\"\n",
      "time=\"2019-07-03T12:17:42+01:00\" level=info msg=\"Applying customresourcedefinitions compositecontrollers.metacontroller.k8s.io\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:43+01:00\" level=info msg=\"Creating non-existent customresourcedefinitions compositecontrollers.metacontroller.k8s.io\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:43+01:00\" level=info msg=\"Applying customresourcedefinitions controllerrevisions.metacontroller.k8s.io\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:43+01:00\" level=info msg=\"Creating non-existent customresourcedefinitions controllerrevisions.metacontroller.k8s.io\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:43+01:00\" level=info msg=\"Applying customresourcedefinitions decoratorcontrollers.metacontroller.k8s.io\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:43+01:00\" level=info msg=\"Creating non-existent customresourcedefinitions decoratorcontrollers.metacontroller.k8s.io\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:44+01:00\" level=info msg=\"Applying serviceaccounts kubeflow.meta-controller-service\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:44+01:00\" level=info msg=\"Creating non-existent serviceaccounts kubeflow.meta-controller-service\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:44+01:00\" level=info msg=\"Applying clusterrolebindings meta-controller-cluster-role-binding\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:44+01:00\" level=info msg=\"Creating non-existent clusterrolebindings meta-controller-cluster-role-binding\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:45+01:00\" level=info msg=\"Applying statefulsets kubeflow.metacontroller\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:45+01:00\" level=info msg=\"Creating non-existent statefulsets kubeflow.metacontroller\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:45+01:00\" level=info msg=\"Component metacontroller apply succeeded\" filename=\"ksonnet/ksonnet.go:171\"\n",
      "time=\"2019-07-03T12:17:54+01:00\" level=info msg=\"Applying customresourcedefinitions notebooks.kubeflow.org\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:54+01:00\" level=info msg=\"Creating non-existent customresourcedefinitions notebooks.kubeflow.org\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:55+01:00\" level=info msg=\"Applying services kubeflow.notebooks-controller\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:55+01:00\" level=info msg=\"Creating non-existent services kubeflow.notebooks-controller\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:55+01:00\" level=info msg=\"Applying serviceaccounts kubeflow.notebook-controller\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:56+01:00\" level=info msg=\"Creating non-existent serviceaccounts kubeflow.notebook-controller\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:56+01:00\" level=info msg=\"Applying deployments kubeflow.notebooks-controller\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:56+01:00\" level=info msg=\"Creating non-existent deployments kubeflow.notebooks-controller\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:57+01:00\" level=info msg=\"Applying clusterroles notebooks-controller\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:57+01:00\" level=info msg=\"Creating non-existent clusterroles notebooks-controller\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:57+01:00\" level=info msg=\"Applying clusterrolebindings notebooks-controller\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:17:57+01:00\" level=info msg=\"Creating non-existent clusterrolebindings notebooks-controller\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:17:57+01:00\" level=info msg=\"Component notebook-controller apply succeeded\" filename=\"ksonnet/ksonnet.go:171\"\n",
      "time=\"2019-07-03T12:18:06+01:00\" level=info msg=\"Applying services kubeflow.minio-service\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:07+01:00\" level=info msg=\"Creating non-existent services kubeflow.minio-service\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:07+01:00\" level=info msg=\"Applying deployments kubeflow.minio\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:07+01:00\" level=info msg=\"Creating non-existent deployments kubeflow.minio\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:08+01:00\" level=info msg=\"Applying secrets kubeflow.mlpipeline-minio-artifact\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:08+01:00\" level=info msg=\"Creating non-existent secrets kubeflow.mlpipeline-minio-artifact\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:08+01:00\" level=info msg=\"Applying services kubeflow.mysql\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:08+01:00\" level=info msg=\"Creating non-existent services kubeflow.mysql\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:09+01:00\" level=info msg=\"Applying deployments kubeflow.mysql\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:09+01:00\" level=info msg=\"Creating non-existent deployments kubeflow.mysql\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:09+01:00\" level=info msg=\"Applying serviceaccounts kubeflow.ml-pipeline\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:09+01:00\" level=info msg=\"Creating non-existent serviceaccounts kubeflow.ml-pipeline\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:10+01:00\" level=info msg=\"Applying rolebindings kubeflow.ml-pipeline\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:10+01:00\" level=info msg=\"Creating non-existent rolebindings kubeflow.ml-pipeline\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:10+01:00\" level=info msg=\"Applying roles kubeflow.ml-pipeline\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:11+01:00\" level=info msg=\"Creating non-existent roles kubeflow.ml-pipeline\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:11+01:00\" level=info msg=\"Applying services kubeflow.ml-pipeline\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:11+01:00\" level=info msg=\"Creating non-existent services kubeflow.ml-pipeline\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:11+01:00\" level=info msg=\"Applying deployments kubeflow.ml-pipeline\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:12+01:00\" level=info msg=\"Creating non-existent deployments kubeflow.ml-pipeline\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:12+01:00\" level=info msg=\"Applying serviceaccounts kubeflow.pipeline-runner\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:12+01:00\" level=info msg=\"Creating non-existent serviceaccounts kubeflow.pipeline-runner\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:12+01:00\" level=info msg=\"Applying clusterroles pipeline-runner\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:13+01:00\" level=info msg=\"Creating non-existent clusterroles pipeline-runner\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:13+01:00\" level=info msg=\"Applying clusterrolebindings pipeline-runner\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:13+01:00\" level=info msg=\"Creating non-existent clusterrolebindings pipeline-runner\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:13+01:00\" level=info msg=\"Applying serviceaccounts kubeflow.ml-pipeline-scheduledworkflow\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:14+01:00\" level=info msg=\"Creating non-existent serviceaccounts kubeflow.ml-pipeline-scheduledworkflow\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:14+01:00\" level=info msg=\"Applying clusterrolebindings ml-pipeline-scheduledworkflow\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:14+01:00\" level=info msg=\"Creating non-existent clusterrolebindings ml-pipeline-scheduledworkflow\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:14+01:00\" level=info msg=\"Applying roles kubeflow.ml-pipeline-scheduledworkflow\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:15+01:00\" level=info msg=\"Creating non-existent roles kubeflow.ml-pipeline-scheduledworkflow\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:15+01:00\" level=info msg=\"Applying deployments kubeflow.ml-pipeline-scheduledworkflow\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:15+01:00\" level=info msg=\"Creating non-existent deployments kubeflow.ml-pipeline-scheduledworkflow\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:15+01:00\" level=info msg=\"Applying customresourcedefinitions scheduledworkflows.kubeflow.org\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:15+01:00\" level=info msg=\"Creating non-existent customresourcedefinitions scheduledworkflows.kubeflow.org\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:16+01:00\" level=info msg=\"Applying serviceaccounts kubeflow.ml-pipeline-persistenceagent\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:16+01:00\" level=info msg=\"Creating non-existent serviceaccounts kubeflow.ml-pipeline-persistenceagent\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:16+01:00\" level=info msg=\"Applying clusterrolebindings ml-pipeline-persistenceagent\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:16+01:00\" level=info msg=\"Creating non-existent clusterrolebindings ml-pipeline-persistenceagent\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:17+01:00\" level=info msg=\"Applying clusterroles ml-pipeline-persistenceagent\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:17+01:00\" level=info msg=\"Creating non-existent clusterroles ml-pipeline-persistenceagent\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:17+01:00\" level=info msg=\"Applying deployments kubeflow.ml-pipeline-persistenceagent\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:17+01:00\" level=info msg=\"Creating non-existent deployments kubeflow.ml-pipeline-persistenceagent\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:18+01:00\" level=info msg=\"Applying serviceaccounts kubeflow.ml-pipeline-viewer-crd-service-account\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:18+01:00\" level=info msg=\"Creating non-existent serviceaccounts kubeflow.ml-pipeline-viewer-crd-service-account\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:18+01:00\" level=info msg=\"Applying clusterrolebindings ml-pipeline-viewer-crd-role-binding\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:18+01:00\" level=info msg=\"Creating non-existent clusterrolebindings ml-pipeline-viewer-crd-role-binding\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:18+01:00\" level=info msg=\"Applying clusterroles ml-pipeline-viewer-controller-role\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:19+01:00\" level=info msg=\"Creating non-existent clusterroles ml-pipeline-viewer-controller-role\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:19+01:00\" level=info msg=\"Applying deployments kubeflow.ml-pipeline-viewer-controller-deployment\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:19+01:00\" level=info msg=\"Creating non-existent deployments kubeflow.ml-pipeline-viewer-controller-deployment\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:19+01:00\" level=info msg=\"Applying customresourcedefinitions viewers.kubeflow.org\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:20+01:00\" level=info msg=\"Creating non-existent customresourcedefinitions viewers.kubeflow.org\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:20+01:00\" level=info msg=\"Applying serviceaccounts kubeflow.ml-pipeline-ui\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:20+01:00\" level=info msg=\"Creating non-existent serviceaccounts kubeflow.ml-pipeline-ui\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:20+01:00\" level=info msg=\"Applying services kubeflow.ml-pipeline-ui\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:21+01:00\" level=info msg=\"Creating non-existent services kubeflow.ml-pipeline-ui\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:21+01:00\" level=info msg=\"Applying services kubeflow.ml-pipeline-tensorboard-ui\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:21+01:00\" level=info msg=\"Creating non-existent services kubeflow.ml-pipeline-tensorboard-ui\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:21+01:00\" level=info msg=\"Applying rolebindings kubeflow.ml-pipeline-ui\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:22+01:00\" level=info msg=\"Creating non-existent rolebindings kubeflow.ml-pipeline-ui\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:22+01:00\" level=info msg=\"Applying roles kubeflow.ml-pipeline-ui\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:22+01:00\" level=info msg=\"Creating non-existent roles kubeflow.ml-pipeline-ui\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:23+01:00\" level=info msg=\"Applying deployments kubeflow.ml-pipeline-ui\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:23+01:00\" level=info msg=\"Creating non-existent deployments kubeflow.ml-pipeline-ui\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:23+01:00\" level=info msg=\"Applying persistentvolumeclaims kubeflow.mysql-pv-claim\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:23+01:00\" level=info msg=\"Creating non-existent persistentvolumeclaims kubeflow.mysql-pv-claim\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:24+01:00\" level=info msg=\"Applying persistentvolumeclaims kubeflow.minio-pvc\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:24+01:00\" level=info msg=\"Creating non-existent persistentvolumeclaims kubeflow.minio-pvc\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:24+01:00\" level=info msg=\"Component pipeline apply succeeded\" filename=\"ksonnet/ksonnet.go:171\"\n",
      "time=\"2019-07-03T12:18:34+01:00\" level=info msg=\"Applying configmaps kubeflow.pytorch-operator-config\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:34+01:00\" level=info msg=\"Creating non-existent configmaps kubeflow.pytorch-operator-config\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:34+01:00\" level=info msg=\"Applying serviceaccounts kubeflow.pytorch-operator\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:35+01:00\" level=info msg=\"Creating non-existent serviceaccounts kubeflow.pytorch-operator\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:35+01:00\" level=info msg=\"Applying clusterroles pytorch-operator\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:35+01:00\" level=info msg=\"Creating non-existent clusterroles pytorch-operator\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:36+01:00\" level=info msg=\"Applying clusterrolebindings pytorch-operator\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:36+01:00\" level=info msg=\"Creating non-existent clusterrolebindings pytorch-operator\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:36+01:00\" level=info msg=\"Applying customresourcedefinitions pytorchjobs.kubeflow.org\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:36+01:00\" level=info msg=\"Creating non-existent customresourcedefinitions pytorchjobs.kubeflow.org\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:37+01:00\" level=info msg=\"Applying deployments kubeflow.pytorch-operator\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:37+01:00\" level=info msg=\"Creating non-existent deployments kubeflow.pytorch-operator\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:37+01:00\" level=info msg=\"Component pytorch-operator apply succeeded\" filename=\"ksonnet/ksonnet.go:171\"\n",
      "time=\"2019-07-03T12:18:47+01:00\" level=info msg=\"Applying services kubeflow.tensorboard\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:47+01:00\" level=info msg=\"Creating non-existent services kubeflow.tensorboard\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:47+01:00\" level=info msg=\"Applying deployments kubeflow.tensorboard\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:47+01:00\" level=info msg=\"Creating non-existent deployments kubeflow.tensorboard\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:48+01:00\" level=info msg=\"Component tensorboard apply succeeded\" filename=\"ksonnet/ksonnet.go:171\"\n",
      "time=\"2019-07-03T12:18:58+01:00\" level=info msg=\"Applying customresourcedefinitions tfjobs.kubeflow.org\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:58+01:00\" level=info msg=\"Creating non-existent customresourcedefinitions tfjobs.kubeflow.org\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:59+01:00\" level=info msg=\"Applying serviceaccounts kubeflow.tf-job-dashboard\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:59+01:00\" level=info msg=\"Creating non-existent serviceaccounts kubeflow.tf-job-dashboard\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:59+01:00\" level=info msg=\"Applying configmaps kubeflow.tf-job-operator-config\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:18:59+01:00\" level=info msg=\"Creating non-existent configmaps kubeflow.tf-job-operator-config\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:18:59+01:00\" level=info msg=\"Applying serviceaccounts kubeflow.tf-job-operator\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:19:00+01:00\" level=info msg=\"Creating non-existent serviceaccounts kubeflow.tf-job-operator\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:19:00+01:00\" level=info msg=\"Applying clusterroles tf-job-operator\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:19:00+01:00\" level=info msg=\"Creating non-existent clusterroles tf-job-operator\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:19:00+01:00\" level=info msg=\"Applying clusterrolebindings tf-job-operator\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:19:01+01:00\" level=info msg=\"Creating non-existent clusterrolebindings tf-job-operator\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:19:01+01:00\" level=info msg=\"Applying services kubeflow.tf-job-dashboard\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:19:01+01:00\" level=info msg=\"Creating non-existent services kubeflow.tf-job-dashboard\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:19:01+01:00\" level=info msg=\"Applying clusterroles tf-job-dashboard\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:19:02+01:00\" level=info msg=\"Creating non-existent clusterroles tf-job-dashboard\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:19:02+01:00\" level=info msg=\"Applying clusterrolebindings tf-job-dashboard\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:19:02+01:00\" level=info msg=\"Creating non-existent clusterrolebindings tf-job-dashboard\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:19:02+01:00\" level=info msg=\"Applying deployments kubeflow.tf-job-operator\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:19:03+01:00\" level=info msg=\"Creating non-existent deployments kubeflow.tf-job-operator\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:19:03+01:00\" level=info msg=\"Applying deployments kubeflow.tf-job-dashboard\" filename=\"cluster/upsert.go:73\"\n",
      "time=\"2019-07-03T12:19:03+01:00\" level=info msg=\"Creating non-existent deployments kubeflow.tf-job-dashboard\" filename=\"cluster/upsert.go:92\"\n",
      "time=\"2019-07-03T12:19:03+01:00\" level=info msg=\"Component tf-job-operator apply succeeded\" filename=\"ksonnet/ksonnet.go:171\"\n",
      "time=\"2019-07-03T12:19:03+01:00\" level=info msg=\"All components apply succeeded\" filename=\"ksonnet/ksonnet.go:192\"\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd kubeflow-seldon\n",
    "kfctl generate all -V\n",
    "kfctl apply all -V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's run Seldon \n",
    "For this we'll need Helm to be running, so we'll initialise it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$HELM_HOME has been configured at /Users/Seldon/.helm.\n",
      "Warning: Tiller is already installed in the cluster.\n",
      "(Use --client-only to suppress this message, or --upgrade to upgrade Tiller to the current version.)\n",
      "deployment \"tiller-deploy\" successfully rolled out\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "helm init \n",
    "kubectl rollout status deploy/tiller-deploy -n kube-system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once it's running we can now run the installation command for Seldon.\n",
    "\n",
    "As you can see, we are running the Seldon Operator in the Kubeflow namespace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME:   tufted-emu\n",
      "LAST DEPLOYED: Wed Jul  3 12:58:26 2019\n",
      "NAMESPACE: kubeflow\n",
      "STATUS: DEPLOYED\n",
      "\n",
      "RESOURCES:\n",
      "==> v1/ClusterRole\n",
      "NAME                          AGE\n",
      "seldon-operator-manager-role  2s\n",
      "\n",
      "==> v1/ClusterRoleBinding\n",
      "NAME                                 AGE\n",
      "seldon-operator-manager-rolebinding  2s\n",
      "\n",
      "==> v1/Pod(related)\n",
      "NAME                                  READY  STATUS             RESTARTS  AGE\n",
      "seldon-operator-controller-manager-0  0/1    ContainerCreating  0         2s\n",
      "\n",
      "==> v1/Secret\n",
      "NAME                                   TYPE    DATA  AGE\n",
      "seldon-operator-webhook-server-secret  Opaque  0     2s\n",
      "\n",
      "==> v1/Service\n",
      "NAME                                        TYPE       CLUSTER-IP    EXTERNAL-IP  PORT(S)  AGE\n",
      "seldon-operator-controller-manager-service  ClusterIP  10.11.252.68  <none>       443/TCP  2s\n",
      "\n",
      "==> v1/ServiceAccount\n",
      "NAME                             SECRETS  AGE\n",
      "tufted-emu-seldon-core-operator  1        2s\n",
      "\n",
      "==> v1/StatefulSet\n",
      "NAME                                READY  AGE\n",
      "seldon-operator-controller-manager  0/1    2s\n",
      "\n",
      "==> v1beta1/CustomResourceDefinition\n",
      "NAME                                         AGE\n",
      "seldondeployments.machinelearning.seldon.io  2s\n",
      "\n",
      "\n",
      "NOTES:\n",
      "NOTES: TODO\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!helm install seldon-core-operator --namespace kubeflow --repo https://storage.googleapis.com/seldon-charts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check all the Seldon Deployment is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seldon-operator-controller-manager-0                       1/1     Running   1          10s\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pod -n kubeflow | grep seldon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporary fix for Argo image\n",
    "\n",
    "At the time of writing we need to make some updates in the Argo images with the following commands below.\n",
    "\n",
    "(This basically changes the images to the latest ones, otherwise we will get an error when we attach the volume)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.extensions/workflow-controller patched\n",
      "deployment.extensions/ml-pipeline patched\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n kubeflow patch deployments. workflow-controller --patch '{\"spec\": {\"template\": {\"spec\": {\"containers\": [{\"name\": \"workflow-controller\", \"image\": \"argoproj/workflow-controller:v2.3.0-rc3\"}]}}}}'\n",
    "!kubectl -n kubeflow patch deployments. ml-pipeline --patch '{\"spec\": {\"template\": {\"spec\": {\"containers\": [{\"name\": \"ml-pipeline-api-server\", \"image\": \"elikatsis/ml-pipeline-api-server:0.1.18-pick-1289\"}]}}}}'\n",
    "# !kubectl -n kubeflow patch configmaps workflow-controller-configmap --patch '{\"data\": {\"config\": \"{ executorImage: argoproj/argoexec:v2.3.0-rc3,artifactRepository:{s3: {bucket: mlpipeline,keyPrefix: artifacts,endpoint: minio-service.kubeflow:9000,insecure: true,accessKeySecret: {name: mlpipeline-minio-artifact,key: accesskey},secretKeySecret: {name: mlpipeline-minio-artifact,key: secretkey}}}}\" }}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last command you need to run actually needs to be manual as the patch cannot change configmap contents directly\n",
    "\n",
    "You need to run the edit commad and change the executorImage to: `argoproj/argoexec:v2.3.0-rc3`\n",
    "\n",
    "The command should be run from a terminal:\n",
    "\n",
    "```\n",
    "kubectl edit configmaps workflow-controller-configmap -n kubeflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Test and build all our reusable pipeline steps\n",
    "\n",
    "We will start by building each of the components in our ML pipeline. \n",
    "\n",
    "![](img/kubeflow-seldon-nlp-reusable-components.jpg)\n",
    "\n",
    "### Let's first have a look at our clean_text step:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mclean_text\u001b[m\u001b[m         \u001b[34mlr_text_classifier\u001b[m\u001b[m \u001b[34mtfidf_vectorizer\u001b[m\u001b[m\r\n",
      "\u001b[34mdata_downloader\u001b[m\u001b[m    \u001b[34mspacy_tokenize\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls pipeline/pipeline_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in this step, all of the other steps can be found in the `pipeline/pipeline_steps/` folder, and all have the following structure:\n",
    "* `pipeline_step.py` which exposes the functionality through a CLI \n",
    "* `Transformer.py` which transforms the data accordingly\n",
    "* `requirements.txt` which states the python dependencies to run\n",
    "* `build_image.sh` which uses `s2i` to build the image with one line\n",
    "\n",
    "### Let's check out the CLI for clean_text\n",
    "The pipeline_step CLI is the entry point for the kubeflow image as it will be able to pass any relevant parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: pipeline_step.py [OPTIONS]\r\n",
      "\r\n",
      "Options:\r\n",
      "  --in-path TEXT\r\n",
      "  --out-path TEXT\r\n",
      "  --help           Show this message and exit.\r\n"
     ]
    }
   ],
   "source": [
    "!python pipeline/pipeline_steps/clean_text/pipeline_step.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually a very simple file, as we are using the click library to define the commands:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import dill\r\n",
      "import click\r\n",
      "import dill\r\n",
      "try:\r\n",
      "    # Running for tests\r\n",
      "    from .Transformer import Transformer\r\n",
      "except:\r\n",
      "    # Running from CLI\r\n",
      "    from Transformer import Transformer\r\n",
      "\r\n",
      "@click.command()\r\n",
      "@click.option('--in-path', default=\"/mnt/raw_text.data\")\r\n",
      "@click.option('--out-path', default=\"/mnt/clean_text.data\")\r\n",
      "def run_pipeline(in_path, out_path):\r\n",
      "    clean_text_transformer = Transformer()\r\n",
      "    with open(in_path, 'rb') as in_f:\r\n",
      "        x = dill.load(in_f)\r\n",
      "    y = clean_text_transformer.predict(x)\r\n",
      "    with open(out_path, \"wb\") as out_f:\r\n",
      "        dill.dump(y, out_f)\r\n",
      "\r\n",
      "if __name__ == \"__main__\":\r\n",
      "    run_pipeline()\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat pipeline/pipeline_steps/clean_text/pipeline_step.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformer is where the data munging and transformation stage comes in, which will be wrapped by the container and exposed through the Seldon Engine to ensure our pipeline can be used in production.\n",
    "\n",
    "Seldon provides multiple different features, such as abilities to send custom metrics, pre-process / post-process data and more. In this example we will only be exposing the `predict` step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import re \r\n",
      "from html.parser import HTMLParser\r\n",
      "import numpy as np\r\n",
      "import logging\r\n",
      "\r\n",
      "class Transformer():\r\n",
      "    __html_parser = HTMLParser()\r\n",
      "    __uplus_pattern = \\\r\n",
      "        re.compile(\"\\<[uU]\\+(?P<digit>[a-zA-Z0-9]+)\\>\")\r\n",
      "    __markup_link_pattern = \\\r\n",
      "        re.compile(\"\\[(.*)\\]\\((.*)\\)\")\r\n",
      "\r\n",
      "    def predict(self, X, feature_names=[]):\r\n",
      "        logging.warning(X)\r\n",
      "        f = np.vectorize(Transformer.transform_clean_text)\r\n",
      "        X_clean = f(X)\r\n",
      "        logging.warning(X_clean)\r\n",
      "        return X_clean\r\n",
      "\r\n",
      "    def fit(self, X, y=None, **fit_params):\r\n",
      "        return self\r\n",
      "    \r\n",
      "    @staticmethod\r\n",
      "    def transform_clean_text(raw_text):\r\n",
      "        try:\r\n",
      "            decoded = raw_text.encode(\"ISO-8859-1\").decode(\"utf-8\")\r\n",
      "        except:\r\n",
      "            decoded = raw_text.encode(\"ISO-8859-1\").decode(\"cp1252\")\r\n",
      "        html_unescaped =Transformer.\\\r\n",
      "            __html_parser.unescape(decoded) \r\n",
      "        html_unescaped = re.sub(r\"\\r\\n\", \" \", html_unescaped)\r\n",
      "        html_unescaped = re.sub(r\"\\r\\r\\n\", \" \", html_unescaped)\r\n",
      "        html_unescaped = re.sub(r\"\\r\", \" \", html_unescaped)\r\n",
      "        html_unescaped = html_unescaped.replace(\"&gt;\", \" > \")\r\n",
      "        html_unescaped = html_unescaped.replace(\"&lt;\", \" < \")\r\n",
      "        html_unescaped = html_unescaped.replace(\"--\", \" - \")\r\n",
      "        html_unescaped = Transformer.__uplus_pattern.sub(\r\n",
      "            \" U\\g<digit> \", html_unescaped)\r\n",
      "        html_unescaped = Transformer.__markup_link_pattern.sub(\r\n",
      "            \" \\1 \\2 \", html_unescaped)\r\n",
      "        html_unescaped = html_unescaped.replace(\"\\\\\", \"\")\r\n",
      "        return html_unescaped\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat pipeline/pipeline_steps/clean_text/Transformer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to understand how the CLI pipeline talks to each other, have a look at the end to end test in `pipeline/pipeline_tests/`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.7.3, pytest-5.0.0, py-1.8.0, pluggy-0.12.0\n",
      "rootdir: /Users/Seldon/seldon-core/examples/kubeflow\n",
      "collected 1 item                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "pipeline/pipeline_tests/test_pipeline.py \u001b[32m.\u001b[0m\u001b[36m                               [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m\u001b[1m===================== 1 passed, 8 warnings in 5.85 seconds =====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest ./pipeline/pipeline_tests/. --disable-pytest-warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the image we provide a build script in each of the steps that contains the instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\r\n",
      "\r\n",
      "s2i build . seldonio/seldon-core-s2i-python3:0.6 clean_text_transformer:0.1\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat pipeline/pipeline_steps/clean_text/build_image.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing you need to make sure is that Seldon knows how to wrap the right model and file.\n",
    "\n",
    "This can be achieved with the s2i/environment file. \n",
    "\n",
    "As you can see, here we just tell it we want it to use our `Transformer.py` file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_NAME=Transformer\r\n",
      "API_TYPE=REST\r\n",
      "SERVICE_TYPE=MODEL\r\n",
      "PERSISTENCE=0\r\n"
     ]
    }
   ],
   "source": [
    "!cat pipeline/pipeline_steps/clean_text/.s2i/environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this is defined, the only thing we need to do is to run the `build_image.sh` for all the reusable components.\n",
    "\n",
    "Here we show the manual way to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  9.728kB\r",
      "\r\n",
      "Step 1/4 : FROM python:3.7-slim\n",
      " ---> 338ae06dfca5\n",
      "Step 2/4 : COPY . /microservice\n",
      " ---> Using cache\n",
      " ---> 1e0524c3d5ac\n",
      "Step 3/4 : WORKDIR /microservice\n",
      " ---> Using cache\n",
      " ---> 917a655f2a90\n",
      "Step 4/4 : RUN pip install -r requirements.txt\n",
      " ---> Using cache\n",
      " ---> 5cb1187d12f1\n",
      "Successfully built 5cb1187d12f1\n",
      "Successfully tagged data_downloader:0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "---> Installing application source...\n",
      "---> Installing dependencies ...\n",
      "Looking in links: /whl\n",
      "Collecting dill==0.2.9 (from -r requirements.txt (line 1))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/fe/42/bfe2e0857bc284cbe6a011d93f2a9ad58a22cb894461b199ae72cfef0f29/dill-0.2.9.tar.gz (150kB)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (7.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (1.16.3)\n",
      "Building wheels for collected packages: dill\n",
      "Building wheel for dill (setup.py): started\n",
      "Building wheel for dill (setup.py): finished with status 'done'\n",
      "Stored in directory: /root/.cache/pip/wheels/5b/d7/0f/e58eae695403de585269f4e4a94e0cd6ca60ec0c202936fa4a\n",
      "Successfully built dill\n",
      "Installing collected packages: dill\n",
      "Successfully installed dill-0.2.9\n",
      "WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "WARNING: You are using pip version 19.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "Build completed successfully\n",
      "---> Installing application source...\n",
      "---> Installing dependencies ...\n",
      "Looking in links: /whl\n",
      "Collecting dill==0.2.9 (from -r requirements.txt (line 1))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/fe/42/bfe2e0857bc284cbe6a011d93f2a9ad58a22cb894461b199ae72cfef0f29/dill-0.2.9.tar.gz (150kB)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (7.0)\n",
      "Requirement already satisfied: numpy==1.16.3 in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (1.16.3)\n",
      "Collecting scikit-learn==0.20.3 (from -r requirements.txt (line 4))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/5e/82/c0de5839d613b82bddd088599ac0bbfbbbcbd8ca470680658352d2c435bd/scikit_learn-0.20.3-cp36-cp36m-manylinux1_x86_64.whl (5.4MB)\n",
      "Collecting scipy>=0.13.3 (from scikit-learn==0.20.3->-r requirements.txt (line 4))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/72/4c/5f81e7264b0a7a8bd570810f48cd346ba36faedbd2ba255c873ad556de76/scipy-1.3.0-cp36-cp36m-manylinux1_x86_64.whl (25.2MB)\n",
      "Building wheels for collected packages: dill\n",
      "Building wheel for dill (setup.py): started\n",
      "Building wheel for dill (setup.py): finished with status 'done'\n",
      "Stored in directory: /root/.cache/pip/wheels/5b/d7/0f/e58eae695403de585269f4e4a94e0cd6ca60ec0c202936fa4a\n",
      "Successfully built dill\n",
      "Installing collected packages: dill, scipy, scikit-learn\n",
      "Successfully installed dill-0.2.9 scikit-learn-0.20.3 scipy-1.3.0\n",
      "WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "WARNING: You are using pip version 19.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "Build completed successfully\n",
      "---> Installing application source...\n",
      "---> Installing dependencies ...\n",
      "Looking in links: /whl\n",
      "Collecting dill==0.2.9 (from -r requirements.txt (line 1))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/fe/42/bfe2e0857bc284cbe6a011d93f2a9ad58a22cb894461b199ae72cfef0f29/dill-0.2.9.tar.gz (150kB)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (7.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (1.16.4)\n",
      "Collecting spacy (from -r requirements.txt (line 4))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/04/44/d260185f41e5d1a43878e63b754befabf4178893af83df5c7e8a95cbd9bd/spacy-2.1.4-cp37-cp37m-manylinux1_x86_64.whl (29.8MB)\n",
      "Collecting wasabi<1.1.0,>=0.2.0 (from spacy->-r requirements.txt (line 4))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/f4/c1/d76ccdd12c716be79162d934fe7de4ac8a318b9302864716dde940641a79/wasabi-0.2.2-py3-none-any.whl\n",
      "Collecting preshed<2.1.0,>=2.0.1 (from spacy->-r requirements.txt (line 4))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/bc/2b/3ecd5d90d2d6fd39fbc520de7d80db5d74defdc2d7c2e15531d9cc3498c7/preshed-2.0.1-cp37-cp37m-manylinux1_x86_64.whl (82kB)\n",
      "Collecting plac<1.0.0,>=0.9.6 (from spacy->-r requirements.txt (line 4))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
      "Collecting thinc<7.1.0,>=7.0.2 (from spacy->-r requirements.txt (line 4))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/4c/4b/64e3b9f5f3a57e38067a5d6c1553ba1eb2c9e3f61b2e183977dc34e71f05/thinc-7.0.4-cp37-cp37m-manylinux1_x86_64.whl (2.1MB)\n",
      "Collecting jsonschema<3.1.0,>=2.6.0 (from spacy->-r requirements.txt (line 4))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/aa/69/df679dfbdd051568b53c38ec8152a3ab6bc533434fc7ed11ab034bf5e82f/jsonschema-3.0.1-py2.py3-none-any.whl (54kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy->-r requirements.txt (line 4))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/73/fc/10eeacb926ec1e88cd62f79d9ac106b0a3e3fe5ff1690422d88c29bd0909/murmurhash-1.0.2-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting blis<0.3.0,>=0.2.2 (from spacy->-r requirements.txt (line 4))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/fa/5f/47b7b29ad202b2210020e2f33bfb06d1db2abe0e709c2a84736e8a9d1bd5/blis-0.2.4-cp37-cp37m-manylinux1_x86_64.whl (3.2MB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy->-r requirements.txt (line 4))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/65/26/e534148e509cbebbea3ee29f50f59eb206621d12c35e4594507da8dc54cc/cymem-2.0.2-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 4)) (2.22.0)\n",
      "Collecting srsly<1.1.0,>=0.0.5 (from spacy->-r requirements.txt (line 4))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/f9/d2/75ed228767b20b1109f52d08d6db2f4c4ca33c5ab55f0d7f0422a390dd02/srsly-0.0.7-cp37-cp37m-manylinux1_x86_64.whl (180kB)\n",
      "Collecting tqdm<5.0.0,>=4.10.0 (from thinc<7.1.0,>=7.0.2->spacy->-r requirements.txt (line 4))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/9f/3d/7a6b68b631d2ab54975f3a4863f3c4e9b26445353264ef01f465dc9b0208/tqdm-4.32.2-py2.py3-none-any.whl (50kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from jsonschema<3.1.0,>=2.6.0->spacy->-r requirements.txt (line 4)) (41.0.1)\n",
      "Collecting attrs>=17.4.0 (from jsonschema<3.1.0,>=2.6.0->spacy->-r requirements.txt (line 4))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/23/96/d828354fa2dbdf216eaa7b7de0db692f12c234f7ef888cc14980ef40d1d2/attrs-19.1.0-py2.py3-none-any.whl\n",
      "Collecting pyrsistent>=0.14.0 (from jsonschema<3.1.0,>=2.6.0->spacy->-r requirements.txt (line 4))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/68/0b/f514e76b4e074386b60cfc6c8c2d75ca615b81e415417ccf3fac80ae0bf6/pyrsistent-0.15.2.tar.gz (106kB)\n",
      "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/site-packages (from jsonschema<3.1.0,>=2.6.0->spacy->-r requirements.txt (line 4)) (1.12.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 4)) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 4)) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 4)) (1.25.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 4)) (2019.6.16)\n",
      "Building wheels for collected packages: dill, pyrsistent\n",
      "Building wheel for dill (setup.py): started\n",
      "Building wheel for dill (setup.py): finished with status 'done'\n",
      "Stored in directory: /root/.cache/pip/wheels/5b/d7/0f/e58eae695403de585269f4e4a94e0cd6ca60ec0c202936fa4a\n",
      "Building wheel for pyrsistent (setup.py): started\n",
      "Building wheel for pyrsistent (setup.py): finished with status 'done'\n",
      "Stored in directory: /root/.cache/pip/wheels/6b/b9/15/c8c6a1e095a370e8c3273e65a5c982e5cf355dde16d77502f5\n",
      "Successfully built dill pyrsistent\n",
      "Installing collected packages: dill, wasabi, cymem, preshed, plac, tqdm, blis, murmurhash, srsly, thinc, attrs, pyrsistent, jsonschema, spacy\n",
      "Successfully installed attrs-19.1.0 blis-0.2.4 cymem-2.0.2 dill-0.2.9 jsonschema-3.0.1 murmurhash-1.0.2 plac-0.9.6 preshed-2.0.1 pyrsistent-0.15.2 spacy-2.1.4 srsly-0.0.7 thinc-7.0.4 tqdm-4.32.2 wasabi-0.2.2\n",
      "Build completed successfully\n",
      "---> Installing application source...\n",
      "---> Installing dependencies ...\n",
      "Looking in links: /whl\n",
      "Collecting dill==0.2.9 (from -r requirements.txt (line 1))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/fe/42/bfe2e0857bc284cbe6a011d93f2a9ad58a22cb894461b199ae72cfef0f29/dill-0.2.9.tar.gz (150kB)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (7.0)\n",
      "Requirement already satisfied: numpy==1.16.3 in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (1.16.3)\n",
      "Collecting scikit-learn==0.20.3 (from -r requirements.txt (line 4))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/5e/82/c0de5839d613b82bddd088599ac0bbfbbbcbd8ca470680658352d2c435bd/scikit_learn-0.20.3-cp36-cp36m-manylinux1_x86_64.whl (5.4MB)\n",
      "Collecting scipy>=0.13.3 (from scikit-learn==0.20.3->-r requirements.txt (line 4))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/72/4c/5f81e7264b0a7a8bd570810f48cd346ba36faedbd2ba255c873ad556de76/scipy-1.3.0-cp36-cp36m-manylinux1_x86_64.whl (25.2MB)\n",
      "Building wheels for collected packages: dill\n",
      "Building wheel for dill (setup.py): started\n",
      "Building wheel for dill (setup.py): finished with status 'done'\n",
      "Stored in directory: /root/.cache/pip/wheels/5b/d7/0f/e58eae695403de585269f4e4a94e0cd6ca60ec0c202936fa4a\n",
      "Successfully built dill\n",
      "Installing collected packages: dill, scipy, scikit-learn\n",
      "Successfully installed dill-0.2.9 scikit-learn-0.20.3 scipy-1.3.0\n",
      "WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "WARNING: You are using pip version 19.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "Build completed successfully\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# we must be in the same directory\n",
    "cd pipeline/pipeline_steps/clean_text/ && ./build_image.sh\n",
    "cd ../data_downloader && ./build_image.sh\n",
    "cd ../lr_text_classifier && ./build_image.sh\n",
    "cd ../spacy_tokenize && ./build_image.sh\n",
    "cd ../tfidf_vectorizer && ./build_image.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Train our NLP Pipeline through the Kubeflow UI\n",
    "We can access the Kubeflow dashboard to train our ML pipeline via http://localhost/_/pipeline-dashboard\n",
    "\n",
    "If you can't edit this, you need to make sure that the ambassador gateway service is accessible:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME         TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE\r\n",
      "ambassador   NodePort   10.11.245.81   <none>        80:31161/TCP   3h56m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get svc ambassador -n kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my case, I need to change the kind from `NodePort` into `LoadBalancer` which can be done with the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service/ambassador patched\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl patch svc ambassador --type='json' -p '[{\"op\":\"replace\",\"path\":\"/spec/type\",\"value\":\"LoadBalancer\"}]' -n kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I've changed it to a loadbalancer, it has allocated the external IP as my localhost so I can access it at http://localhost/_/pipeline-dashboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME         TYPE           CLUSTER-IP     EXTERNAL-IP       PORT(S)        AGE\r\n",
      "ambassador   LoadBalancer   10.11.245.81   104.154.166.116   80:31161/TCP   3h57m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get svc ambassador -n kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this was successfull, you should be able to access the dashboard\n",
    "![](img/k-pipeline-dashboard.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the pipeline\n",
    "Now we want to generate the pipeline. For this we can use the DSL provided by kubeflow to define the actual steps required. \n",
    "\n",
    "The pipeline will look as follows:\n",
    "\n",
    "![](img/kubeflow-seldon-nlp-ml-pipelines.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "import kfp.dsl as dsl\r\n",
      "import yaml\r\n",
      "from kubernetes import client as k8s\r\n",
      "\r\n",
      "\r\n",
      "@dsl.pipeline(\r\n",
      "  name='NLP',\r\n",
      "  description='A pipeline demonstrating reproducible steps for NLP'\r\n",
      ")\r\n",
      "def nlp_pipeline(\r\n",
      "        csv_url=\"https://raw.githubusercontent.com/axsauze/reddit-classification-exploration/master/data/reddit_train.csv\",\r\n",
      "        csv_encoding=\"ISO-8859-1\",\r\n",
      "        features_column=\"BODY\",\r\n",
      "        labels_column=\"REMOVED\",\r\n",
      "        raw_text_path='/mnt/text.data',\r\n",
      "        labels_path='/mnt/labels.data',\r\n",
      "        clean_text_path='/mnt/clean.data',\r\n",
      "        spacy_tokens_path='/mnt/tokens.data',\r\n",
      "        tfidf_vectors_path='/mnt/tfidf.data',\r\n",
      "        lr_prediction_path='/mnt/prediction.data',\r\n",
      "        tfidf_model_path='/mnt/tfidf.model',\r\n",
      "        lr_model_path='/mnt/lr.model',\r\n",
      "        lr_c_param=0.1,\r\n",
      "        tfidf_max_features=10000,\r\n",
      "        tfidf_ngram_range=3,\r\n",
      "        batch_size='100'):\r\n",
      "    \"\"\"\r\n",
      "    Pipeline \r\n",
      "    \"\"\"\r\n",
      "    vop = dsl.VolumeOp(\r\n",
      "      name='my-pvc',\r\n",
      "      resource_name=\"my-pvc\",\r\n",
      "      modes=[\"ReadWriteMany\"],\r\n",
      "      storage_class=\"nfs-client\",\r\n",
      "      size=\"1Gi\"\r\n",
      "    )\r\n",
      "\r\n",
      "    download_step = dsl.ContainerOp(\r\n",
      "        name='data_downloader',\r\n",
      "        image='gcr.io/dev-joel/data_downloader:0.1',\r\n",
      "        command=\"python\",\r\n",
      "        arguments=[\r\n",
      "            \"/microservice/pipeline_step.py\",\r\n",
      "            \"--labels-path\", labels_path,\r\n",
      "            \"--features-path\", raw_text_path,\r\n",
      "            \"--csv-url\", csv_url,\r\n",
      "            \"--csv-encoding\", csv_encoding,\r\n",
      "            \"--features-column\", features_column,\r\n",
      "            \"--labels-column\", labels_column\r\n",
      "        ],\r\n",
      "        pvolumes={\"/mnt\": vop.volume}\r\n",
      "    )\r\n",
      "\r\n",
      "    clean_step = dsl.ContainerOp(\r\n",
      "        name='clean_text',\r\n",
      "        image='gcr.io/dev-joel/clean_text_transformer:0.1',\r\n",
      "        command=\"python\",\r\n",
      "        arguments=[\r\n",
      "            \"/microservice/pipeline_step.py\",\r\n",
      "            \"--in-path\", raw_text_path,\r\n",
      "            \"--out-path\", clean_text_path,\r\n",
      "        ],\r\n",
      "        pvolumes={\"/mnt\": download_step.pvolume}\r\n",
      "    )\r\n",
      "\r\n",
      "    tokenize_step = dsl.ContainerOp(\r\n",
      "        name='tokenize',\r\n",
      "        image='gcr.io/dev-joel/spacy_tokenizer:0.1',\r\n",
      "        command=\"python\",\r\n",
      "        arguments=[\r\n",
      "            \"/microservice/pipeline_step.py\",\r\n",
      "            \"--in-path\", clean_text_path,\r\n",
      "            \"--out-path\", spacy_tokens_path,\r\n",
      "        ],\r\n",
      "        pvolumes={\"/mnt\": clean_step.pvolume}\r\n",
      "    )\r\n",
      "\r\n",
      "    vectorize_step = dsl.ContainerOp(\r\n",
      "        name='vectorize',\r\n",
      "        image='gcr.io/dev-joel/tfidf_vectorizer:0.1',\r\n",
      "        command=\"python\",\r\n",
      "        arguments=[\r\n",
      "            \"/microservice/pipeline_step.py\",\r\n",
      "            \"--in-path\", spacy_tokens_path,\r\n",
      "            \"--out-path\", tfidf_vectors_path,\r\n",
      "            \"--max-features\", tfidf_max_features,\r\n",
      "            \"--ngram-range\", tfidf_ngram_range,\r\n",
      "            \"--action\", \"train\",\r\n",
      "            \"--model-path\", tfidf_model_path,\r\n",
      "        ],\r\n",
      "        pvolumes={\"/mnt\": tokenize_step.pvolume}\r\n",
      "    )\r\n",
      "\r\n",
      "    predict_step = dsl.ContainerOp(\r\n",
      "        name='predictor',\r\n",
      "        image='gcr.io/dev-joel/lr_text_classifier:0.1',\r\n",
      "        command=\"python\",\r\n",
      "        arguments=[\r\n",
      "            \"/microservice/pipeline_step.py\",\r\n",
      "            \"--in-path\", tfidf_vectors_path,\r\n",
      "            \"--labels-path\", labels_path,\r\n",
      "            \"--out-path\", lr_prediction_path,\r\n",
      "            \"--c-param\", lr_c_param,\r\n",
      "            \"--action\", \"train\",\r\n",
      "            \"--model-path\", lr_model_path,\r\n",
      "        ],\r\n",
      "        pvolumes={\"/mnt\": vectorize_step.pvolume}\r\n",
      "    )\r\n",
      "\r\n",
      "    try:\r\n",
      "        seldon_config = yaml.load(open(\"../deploy_pipeline/seldon_production_pipeline.yaml\"), Loader=yaml.FullLoader)\r\n",
      "    except:\r\n",
      "        # If this file is run from the project core directory \r\n",
      "        seldon_config = yaml.load(open(\"deploy_pipeline/seldon_production_pipeline.yaml\"), Loader=yaml.FullLoader)\r\n",
      "\r\n",
      "    deploy_step = dsl.ResourceOp(\r\n",
      "        name=\"seldondeploy\",\r\n",
      "        k8s_resource=seldon_config,\r\n",
      "        attribute_outputs={\"name\": \"{.metadata.name}\"})\r\n",
      "\r\n",
      "    deploy_step.after(predict_step)\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "  import kfp.compiler as compiler\r\n",
      "  compiler.Compiler().compile(nlp_pipeline, __file__ + '.tar.gz')\r\n"
     ]
    }
   ],
   "source": [
    "!cat train_pipeline/nlp_pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breaking down the  code\n",
    "As you can see in the DSL, we have the ContainerOp - each of those is a step in the Kubeflow pipeline.\n",
    "\n",
    "At the end we can see the `seldondeploy` step which basically deploys the trained pipeline\n",
    "\n",
    "The definition of the SeldonDeployment graph is provided in the `deploy_pipeline/seldon_production_pipeline.yaml` file.\n",
    "\n",
    "The seldondeployment file defines our production execution graph using the same reusable components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\r\n",
      "apiVersion: machinelearning.seldon.io/v1alpha2\r\n",
      "kind: SeldonDeployment\r\n",
      "metadata:\r\n",
      "  labels:\r\n",
      "    app: seldon\r\n",
      "  name: \"seldon-deployment-{{workflow.name}}\"\r\n",
      "  namespace: kubeflow\r\n",
      "spec:\r\n",
      "  annotations:\r\n",
      "    project_name: NLP Pipeline\r\n",
      "    deployment_version: v1\r\n",
      "  name: \"seldon-deployment-{{workflow.name}}\"\r\n",
      "  oauth_key: oauth-key\r\n",
      "  oauth_secret: oauth-secret\r\n",
      "  predictors:\r\n",
      "  - componentSpecs:\r\n",
      "    - spec:\r\n",
      "        containers:\r\n",
      "        - image: gcr.io/dev-joel/clean_text_transformer:0.1\r\n",
      "          imagePullPolicy: IfNotPresent\r\n",
      "          name: cleantext\r\n",
      "          resources:\r\n",
      "            requests:\r\n",
      "              memory: 1Mi\r\n",
      "        - image: gcr.io/dev-joel/spacy_tokenizer:0.1\r\n",
      "          imagePullPolicy: IfNotPresent\r\n",
      "          name: spacytokenizer\r\n",
      "        - image: tfidf_vectorizer:0.1\r\n",
      "          imagePullPolicy: IfNotPresent\r\n",
      "          name: tfidfvectorizer\r\n",
      "          volumeMounts:\r\n",
      "          - name: mypvc\r\n",
      "            mountPath: /mnt\r\n",
      "        - image: gcr.io/dev-joel/lr_text_classifier:0.1\r\n",
      "          imagePullPolicy: IfNotPresent\r\n",
      "          name: lrclassifier\r\n",
      "          volumeMounts:\r\n",
      "          - name: mypvc\r\n",
      "            mountPath: /mnt\r\n",
      "        terminationGracePeriodSeconds: 20\r\n",
      "        volumes:\r\n",
      "        - name: mypvc\r\n",
      "          persistentVolumeClaim:\r\n",
      "            claimName: \"{{workflow.name}}-my-pvc\"\r\n",
      "    graph:\r\n",
      "      children:\r\n",
      "      - name: spacytokenizer\r\n",
      "        endpoint:\r\n",
      "          type: REST\r\n",
      "        type: MODEL\r\n",
      "        children:\r\n",
      "        - name: tfidfvectorizer\r\n",
      "          endpoint:\r\n",
      "            type: REST\r\n",
      "          type: MODEL\r\n",
      "          children:\r\n",
      "          - name: lrclassifier\r\n",
      "            endpoint:\r\n",
      "              type: REST\r\n",
      "            type: MODEL\r\n",
      "            children: []\r\n",
      "      name: cleantext\r\n",
      "      endpoint:\r\n",
      "        type: REST\r\n",
      "      type: MODEL\r\n",
      "    name: single-model\r\n",
      "    replicas: 1\r\n",
      "    annotations:\r\n",
      "      predictor_version: v1\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat deploy_pipeline/seldon_production_pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seldon Production pipeline contents\n",
    "If we look at the file we'll be using to deploy our pipeline, we can see that it has the following key points:\n",
    "\n",
    "1) Reusable components definitions as containerSpecs: cleantext, spacytokenizer, tfidfvectorizer & lrclassifier\n",
    "\n",
    "2) DAG (directed acyclic graph) definition for REST pipeline: cleantext -> spacytokenizer -> tfidfvectorizer -> lrclassifier\n",
    "\n",
    "This graph in our production deployment looks as follows:\n",
    "\n",
    "![](img/kubeflow-seldon-nlp-ml-pipelines-deploy.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the pipeline files to upload to Kubeflow\n",
    "To generate the pipeline we just have to run the pipeline file, which will output the `tar.gz` file that will be uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp_pipeline.py\n",
      "nlp_pipeline.py.tar.gz\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Generating graph definition\n",
    "python train_pipeline/nlp_pipeline.py\n",
    "ls train_pipeline/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Run the pipeline\n",
    "\n",
    "We now need to upload the resulting `nlp_pipeline.py.tar.gz` file generated.\n",
    "\n",
    "This can be done through the \"Upload PIpeline\" button in the UI at http://localhost/_/pipeline-dashboard.\n",
    "\n",
    "Once it's uploaded, we want to create and trigger a run! You should now be able to see how each step is executed:\n",
    "\n",
    "![](img/running-pipeline.jpg)\n",
    "\n",
    "### Inspecting the data created in the Persistent Volume\n",
    "The pipeline saves the output of the pipeline together with the trained model in the persistent volume claim.\n",
    "\n",
    "The persistent volume claim is the same name as the argo workflow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME        AGE\r\n",
      "nlp-7zfn2   55m\r\n",
      "nlp-8r495   28m\r\n",
      "nlp-b6n8b   26m\r\n",
      "nlp-c7bx4   23m\r\n",
      "nlp-lcv7v   1h\r\n",
      "nlp-phw95   51m\r\n",
      "nlp-wdz7z   2m\r\n",
      "nlp-wqkhb   16m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get workflow -n kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our workflow is there! So we can actually access it by running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp-7zfn2"
     ]
    }
   ],
   "source": [
    "!kubectl get workflow -n kubeflow -o jsonpath='{.items[0].metadata.name}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can use good old `sed` to insert this workflow name in our PVC-Access controler which we can use to inspect the contents of the volume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: v1\r\n",
      "kind: Pod\r\n",
      "metadata:\r\n",
      "  name: pvc-access-container\r\n",
      "spec:\r\n",
      "  containers:\r\n",
      "  - name: pvc-access-container\r\n",
      "    image: busybox\r\n",
      "    command: [\"/bin/sh\", \"-ec\", \"sleep 1000\"]\r\n",
      "    volumeMounts:\r\n",
      "    - name: mypvc\r\n",
      "      mountPath: /mnt\r\n",
      "  volumes:\r\n",
      "  - name: mypvc\r\n",
      "    persistentVolumeClaim:\r\n",
      "      claimName: nlp-7zfn2-my-pvc\r\n"
     ]
    }
   ],
   "source": [
    "!sed \"s/PVC_NAME/\"$(kubectl get workflow -n kubeflow -o jsonpath='{.items[0].metadata.name}')\"-my-pvc/g\" deploy_pipeline/pvc-access.yaml "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just need to apply this container with our kubectl command, and we can use it to inspect the mounted folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/pvc-access-container created\r\n"
     ]
    }
   ],
   "source": [
    "!sed \"s/PVC_NAME/\"$(kubectl get workflow -n kubeflow -o jsonpath='{.items[0].metadata.name}')\"-my-pvc/g\" deploy_pipeline/pvc-access.yaml | kubectl -n kubeflow apply -f -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                   READY   STATUS    RESTARTS   AGE\r\n",
      "pvc-access-container   1/1     Running   0          6s\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods -n kubeflow pvc-access-container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run an `ls` command to see what's inside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl -n kubeflow exec -it pvc-access-container ls /mnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod \"pvc-access-container\" deleted\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete -f deploy_pipeline/pvc-access.yaml -n kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Test Deployed ML REST Endpoints\n",
    "Now that it's running we have a production ML text pipeline that we can Query using REST and GRPC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can check if our Seldon deployment is running with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                          AGE\r\n",
      "seldon-deployment-nlp-wdz7z   1m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n kubeflow get seldondeployment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need the Seldon Pipeline Deployment name to reach the API, so we can get it using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seldon-deployment-nlp-wdz7z"
     ]
    }
   ],
   "source": [
    "!kubectl -n kubeflow get seldondeployment -o jsonpath='{.items[0].metadata.name}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can interact with our API in two ways: \n",
    "\n",
    "1) Using CURL or any client like PostMan\n",
    "\n",
    "2) Using the Python SeldonClient\n",
    "\n",
    "### Using CURL from the terminal\n",
    "When using CURL, the only thing we need to provide is the data in JSON format, as well as the url, which is of the format:\n",
    "\n",
    "```\n",
    "http://<ENDPOINT>/seldon/kubeflow/<PIPELINE_NAME>/api/v0.1/predictions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to 127.0.0.1 port 80: Connection refused\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'curl -X POST -H \\'Content-Type: application/json\\' \\\\\\n    -d \"{\\'data\\': {\\'names\\': [\\'text\\'], \\'ndarray\\': [\\'Hello world this is a test\\']}}\" \\\\\\n    http://127.0.0.1/seldon/kubeflow/$(kubectl -n kubeflow get seldondeployment -o jsonpath=\\'{.items[0].metadata.name}\\')/api/v0.1/predictions\\n'' returned non-zero exit status 7.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-0a00198e29b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'curl -X POST -H \\'Content-Type: application/json\\' \\\\\\n    -d \"{\\'data\\': {\\'names\\': [\\'text\\'], \\'ndarray\\': [\\'Hello world this is a test\\']}}\" \\\\\\n    http://127.0.0.1/seldon/kubeflow/$(kubectl -n kubeflow get seldondeployment -o jsonpath=\\'{.items[0].metadata.name}\\')/api/v0.1/predictions\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2350\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2352\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2353\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</Users/Seldon/miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-110>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'curl -X POST -H \\'Content-Type: application/json\\' \\\\\\n    -d \"{\\'data\\': {\\'names\\': [\\'text\\'], \\'ndarray\\': [\\'Hello world this is a test\\']}}\" \\\\\\n    http://127.0.0.1/seldon/kubeflow/$(kubectl -n kubeflow get seldondeployment -o jsonpath=\\'{.items[0].metadata.name}\\')/api/v0.1/predictions\\n'' returned non-zero exit status 7."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -X POST -H 'Content-Type: application/json' \\\n",
    "    -d \"{'data': {'names': ['text'], 'ndarray': ['Hello world this is a test']}}\" \\\n",
    "    http://127.0.0.1/seldon/kubeflow/$(kubectl -n kubeflow get seldondeployment -o jsonpath='{.items[0].metadata.name}')/api/v0.1/predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the SeldonClient\n",
    "We can also use the Python SeldonClient to interact with the pipeline we just deployed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"/Users/Seldon/miniconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\n  File \"/Users/Seldon/miniconda3/lib/python3.7/imp.py\", line 296, in find_module\n    raise ImportError(_ERR_MSG.format(name), name=name)\nImportError: No module named '_pywrap_tensorflow_internal'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/Seldon/miniconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/Users/Seldon/miniconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/Users/Seldon/miniconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\n    import _pywrap_tensorflow_internal\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/imp.py\u001b[0m in \u001b[0;36mfind_module\u001b[0;34m(name, path)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ERR_MSG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named '_pywrap_tensorflow_internal'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;32mimport\u001b[0m \u001b[0m_pywrap_tensorflow_internal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_pywrap_tensorflow_internal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '_pywrap_tensorflow_internal'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-d9f7ce1ca4b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mseldon_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseldon_client\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSeldonClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"localhost\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/seldon_core/seldon_client.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mseldon_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprediction_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mseldon_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprediction_pb2_grpc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mseldon_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_to_grpc_datadef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseldon_message_to_json\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mjson_to_seldon_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeedback_to_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseldon_messages_to_json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/seldon_core/proto/prediction_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstruct_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgoogle_dot_protobuf_dot_struct__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_tensor__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Protocol buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[0;32m---> 74\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"/Users/Seldon/miniconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\n  File \"/Users/Seldon/miniconda3/lib/python3.7/imp.py\", line 296, in find_module\n    raise ImportError(_ERR_MSG.format(name), name=name)\nImportError: No module named '_pywrap_tensorflow_internal'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/Seldon/miniconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/Users/Seldon/miniconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/Users/Seldon/miniconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\n    import _pywrap_tensorflow_internal\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "from seldon_core.seldon_client import SeldonClient\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "host = \"localhost\"\n",
    "port = \"80\" # Make sure you use the port above\n",
    "batch = np.array([\"Hello world this is a test\"])\n",
    "payload_type = \"ndarray\"\n",
    "# Get the deployment name\n",
    "deployment_name = subprocess.getoutput(\"kubectl -n kubeflow get seldondeployment -o jsonpath='{.items[0].metadata.name}'\")\n",
    "transport=\"rest\"\n",
    "namespace=\"kubeflow\"\n",
    "\n",
    "sc = SeldonClient(\n",
    "    gateway=\"ambassador\", \n",
    "    ambassador_endpoint=host + \":\" + port,\n",
    "    namespace=namespace)\n",
    "\n",
    "client_prediction = sc.predict(\n",
    "    data=batch, \n",
    "    deployment_name=deployment_name,\n",
    "    names=[\"text\"],\n",
    "    payload_type=payload_type,\n",
    "    transport=\"rest\")\n",
    "\n",
    "print(client_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Visualise Seldon's Production ML Pipelines\n",
    "We can visualise the performance using the SeldonAnalytics package, which we can deploy using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME:   gangly-quail\n",
      "LAST DEPLOYED: Wed Jul  3 17:48:56 2019\n",
      "NAMESPACE: kubeflow\n",
      "STATUS: DEPLOYED\n",
      "\n",
      "RESOURCES:\n",
      "==> v1/ConfigMap\n",
      "NAME                       DATA  AGE\n",
      "alertmanager-server-conf   1     3s\n",
      "grafana-import-dashboards  11    3s\n",
      "prometheus-rules           0     3s\n",
      "prometheus-server-conf     1     3s\n",
      "\n",
      "==> v1/Job\n",
      "NAME                            COMPLETIONS  DURATION  AGE\n",
      "grafana-prom-import-dashboards  0/1          3s        3s\n",
      "\n",
      "==> v1/Pod(related)\n",
      "NAME                                      READY  STATUS             RESTARTS  AGE\n",
      "alertmanager-deployment-79d4f8b64-7fn5k   0/1    ContainerCreating  0         3s\n",
      "grafana-prom-deployment-687bc94bbb-f959w  0/1    ContainerCreating  0         2s\n",
      "grafana-prom-import-dashboards-f5wvt      0/1    ContainerCreating  0         3s\n",
      "prometheus-deployment-68b9445fb8-mjm9f    0/1    ContainerCreating  0         2s\n",
      "prometheus-node-exporter-rhmlf            0/1    ContainerCreating  0         2s\n",
      "\n",
      "==> v1/Secret\n",
      "NAME                 TYPE    DATA  AGE\n",
      "grafana-prom-secret  Opaque  1     3s\n",
      "\n",
      "==> v1/Service\n",
      "NAME                      TYPE       CLUSTER-IP     EXTERNAL-IP  PORT(S)       AGE\n",
      "alertmanager              ClusterIP  10.11.240.244  <none>       80/TCP        3s\n",
      "grafana-prom              NodePort   10.11.246.155  <none>       80:31680/TCP  2s\n",
      "prometheus-node-exporter  ClusterIP  None           <none>       9100/TCP      2s\n",
      "prometheus-seldon         ClusterIP  10.11.249.243  <none>       80/TCP        2s\n",
      "\n",
      "==> v1/ServiceAccount\n",
      "NAME        SECRETS  AGE\n",
      "prometheus  1        3s\n",
      "\n",
      "==> v1beta1/ClusterRole\n",
      "NAME        AGE\n",
      "prometheus  3s\n",
      "\n",
      "==> v1beta1/ClusterRoleBinding\n",
      "NAME        AGE\n",
      "prometheus  3s\n",
      "\n",
      "==> v1beta1/DaemonSet\n",
      "NAME                      DESIRED  CURRENT  READY  UP-TO-DATE  AVAILABLE  NODE SELECTOR  AGE\n",
      "prometheus-node-exporter  1        1        0      1           0          <none>         2s\n",
      "\n",
      "==> v1beta1/Deployment\n",
      "NAME                     READY  UP-TO-DATE  AVAILABLE  AGE\n",
      "alertmanager-deployment  0/1    1           0          3s\n",
      "grafana-prom-deployment  0/1    1           0          3s\n",
      "prometheus-deployment    0/1    1           0          2s\n",
      "\n",
      "\n",
      "NOTES:\n",
      "NOTES: TODO\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!helm install seldon-core-analytics --repo https://storage.googleapis.com/seldon-charts --namespace kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my case, similar to what I did with Ambassador, I need to make sure the the service is a LoadBalancer instead of a NodePort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service/grafana-prom patched\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl patch svc grafana-prom --type='json' -p '[{\"op\":\"replace\",\"path\":\"/spec/type\",\"value\":\"LoadBalancer\"}]' -n kubeflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME           TYPE           CLUSTER-IP      EXTERNAL-IP       PORT(S)        AGE\r\n",
      "grafana-prom   LoadBalancer   10.11.246.155   104.198.131.212   80:31680/TCP   48s\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get svc grafana-prom -n kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can access it at the port provided, in my case it is http://localhost:32445/d/3swM2iGWz/prediction-analytics?refresh=5s&orgId=1\n",
    "\n",
    "(initial username is admin and password is password, which will be requested to be changed on the first login)\n",
    "\n",
    "Generate a bunch of requests and visualise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    client_prediction = sc.predict(\n",
    "        data=batch, \n",
    "        deployment_name=deployment_name,\n",
    "        names=[\"text\"],\n",
    "        payload_type=payload_type,\n",
    "        transport=\"rest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You now have a full end-to-end training and production NLP pipeline ðŸ˜Ž \n",
    "![](img/seldon-analytics.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
